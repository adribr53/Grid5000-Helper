{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD ALL CSV FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build csv files for all xps\n",
    "\n",
    "sender_type = ['sender_rdma', 'sender_rdma_write_multip_write', 'sender_rdma_write_multip_read', 'sender_rdma_write_nomultip_read']\n",
    "payload_sizes = ['10-100','100-1000','1000-10000']\n",
    "simult_req = ['1','10','100']\n",
    "payload_bounds = ['1500']\n",
    "circle_sizes = ['200']\n",
    "\n",
    "global_data = []\n",
    "\n",
    "for size in payload_sizes:\n",
    "   for n in simult_req:\n",
    "    for payload_bound in payload_bounds:\n",
    "            for circle_size in circle_sizes:\n",
    "        # one xp, let's get th latencies and avg req rates\n",
    "        #   CASE   OPS/SEC ...\n",
    "        # sender | res1 res2 res3        \n",
    "                data = []\n",
    "                #For custom senders\n",
    "                for sender in sender_type:\n",
    "                    result_json = f\"results/json_{sender}_{size}_{n}_{payload_bound}_{circle_size}\"\n",
    "                    # print(result_json)            \n",
    "                    with open(result_json, 'r') as file:\n",
    "                        result =  json.load(file)                \n",
    "                        \n",
    "                        result_dict = {}\n",
    "                        result_dict[\"sender_type\"] = sender[7:]\n",
    "                        result_dict[\"payload_size\"] = size\n",
    "                        result_dict[\"simult_req\"] = n\n",
    "                        result_dict[\"payload_bound\"] = payload_bound\n",
    "                        result_dict[\"circle_size\"] = circle_size\n",
    "                        result_dict[\"Ops/sec\"] = result['ALL STATS']['Totals']['Ops/sec']\n",
    "                        result_dict[\"Average\"] = result['ALL STATS']['Totals']['Average Latency']\n",
    "                        result_dict[\"p50.00\"] = result['ALL STATS']['Totals']['Percentile Latencies']['p50.00']\n",
    "                        result_dict[\"p99.00\"] = result['ALL STATS']['Totals']['Percentile Latencies']['p99.00']\n",
    "                        result_dict[\"p99.90\"] = result['ALL STATS']['Totals']['Percentile Latencies']['p99.90']\n",
    "                        \n",
    "                        data.append(result_dict)\n",
    "                        global_data.append(result_dict)\n",
    "                    # print(result)  \n",
    "\n",
    "                #For tcp_proxy    \n",
    "                result_json = f\"results/json_tcpproxy_{size}_{n}\"\n",
    "                # print(result_json)            \n",
    "                with open(result_json, 'r') as file:\n",
    "                    result =  json.load(file)                \n",
    "                    \n",
    "                    result_dict = {}\n",
    "                    result_dict[\"sender_type\"] = \"tcpproxy\"\n",
    "                    result_dict[\"payload_size\"] = size\n",
    "                    result_dict[\"simult_req\"] = n\n",
    "                    result_dict[\"payload_bound\"] = payload_bound\n",
    "                    result_dict[\"circle_size\"] = circle_size\n",
    "                    result_dict[\"Ops/sec\"] = result['ALL STATS']['Totals']['Ops/sec']\n",
    "                    result_dict[\"Average\"] = result['ALL STATS']['Totals']['Average Latency']\n",
    "                    result_dict[\"p50.00\"] = result['ALL STATS']['Totals']['Percentile Latencies']['p50.00']\n",
    "                    result_dict[\"p99.00\"] = result['ALL STATS']['Totals']['Percentile Latencies']['p99.00']\n",
    "                    result_dict[\"p99.90\"] = result['ALL STATS']['Totals']['Percentile Latencies']['p99.90']\n",
    "                    \n",
    "                    data.append(result_dict)\n",
    "                    global_data.append(result_dict)\n",
    "                # print(result)  \n",
    "\n",
    "                # print(\"\\n\\n\\n\")\n",
    "                df = pd.DataFrame(data)\n",
    "                df.to_csv(f'data/res_{size}_{n}_{payload_bound}_{circle_size}.csv')\n",
    "\n",
    "global_df = pd.DataFrame(global_data)\n",
    "global_df.to_csv(f'data/res_global.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build individual metric comparison plots\n",
    "1 dimension x axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build individual metric comparison plots\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "payload_sizes = ['10-100','100-1000','1000-10000']\n",
    "simult_req = ['1','10','100']\n",
    "payload_bounds = ['1500']\n",
    "circle_sizes = ['200']\n",
    "metrics = ['p99.90', 'Average']\n",
    "for size in payload_sizes:\n",
    "   for n in simult_req:\n",
    "    for payload_bound in payload_bounds:\n",
    "            for circle_size in circle_sizes:\n",
    "                for metric in metrics:\n",
    "                    plt.clf() # Reset plot\n",
    "\n",
    "                    # Read the CSV file\n",
    "                    df = pd.read_csv(f\"data/res_{size}_{n}_{payload_bound}_{circle_size}.csv\")\n",
    "\n",
    "                    # Extract the required data\n",
    "                    sender_types = df['sender_type']\n",
    "                    values = df[metric]\n",
    "\n",
    "                    # Set up seaborn style\n",
    "                    sns.set(style=\"whitegrid\")\n",
    "\n",
    "                    # Create a bar plot\n",
    "                    plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "                    sns.barplot(x=sender_types, y=values, palette=\"viridis\")\n",
    "                    plt.xlabel('Prototype', fontsize=12)\n",
    "                    plt.ylabel(f'{metric} Latency (ms)', fontsize=12)\n",
    "                    plt.title(f\"\"\"Comparison of {metric} Latency by Prototype\\n\n",
    "                                for payload_size={size}, simult_req={n}, payload_bound={payload_bound}, circle_size={circle_size}\"\"\"\n",
    "                            , fontsize=14)\n",
    "                    plt.xticks(rotation=45, ha='right', fontsize=10)  # Rotate and align x-axis labels\n",
    "\n",
    "                    # Add data labels\n",
    "                    for index, value in enumerate(values):\n",
    "                        plt.text(index, value, str(round(value, 3)), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "                    # Remove top and right spines\n",
    "                    sns.despine()\n",
    "\n",
    "                    # Display the plot\n",
    "                    plt.tight_layout()  # Adjust spacing\n",
    "                    \n",
    "                    plt.savefig(f'plots/type1_{size}_{n}_{payload_bound}_{circle_size}_{metric}.svg', bbox_inches='tight', format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build (sender_type, param) comparison plots\n",
    "2 dimensions x axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build (sender_type, payload_size) comparison plots\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('data/res_global.csv')\n",
    "\n",
    "simult_req = [1, 10, 100]\n",
    "payload_bounds = [1500]\n",
    "circle_sizes = [200]\n",
    "metrics = ['p99.90', 'p99.00', 'Average']\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Compare average throughput\n",
    "for n in simult_req:\n",
    "    for payload_bound in payload_bounds:\n",
    "        for circle_size in circle_sizes:\n",
    "            for metric in metrics:\n",
    "                plt.clf() # Reset plot\n",
    "                df_fixed = df[(df['simult_req'] == n) & (df['payload_bound'] == payload_bound) & (df['circle_size'] == circle_size)]\n",
    "\n",
    "                df_comp_fixed_temp = df_fixed.pivot(\"sender_type\", \"payload_size\", metric)\n",
    "                ax = df_comp_fixed_temp.plot(kind='bar', rot=0)\n",
    "\n",
    "                # Set plot title and labels\n",
    "                plt.title(f\"{metric} Latency comparison \\n for simult_req={n}, payload_bound={payload_bound}, circle_size={circle_size}\")\n",
    "                plt.xlabel('Protoype')\n",
    "                plt.ylabel(f\"{metric} Latency (ms)\")\n",
    "\n",
    "                # Orient sender_type labels at 45 degrees\n",
    "                ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "                ax.tick_params(axis='x', pad=10)\n",
    "\n",
    "                # Adjust plot layout\n",
    "                plt.legend(title='Payload Size', bbox_to_anchor=(1, 1))\n",
    "                plt.tight_layout()\n",
    "\n",
    "                # Show or save the plot\n",
    "                plt.savefig(f'plots/sender-comp-payload_size-{n}_{payload_bound}_{circle_size}_{metric}.svg', bbox_inches='tight', format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build (sender_type, simult_req) comparison plots\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('data/res_global.csv')\n",
    "\n",
    "payload_sizes = ['10-100','100-1000','1000-10000']\n",
    "payload_bounds = [1500]\n",
    "circle_sizes = [200]\n",
    "metrics = ['p99.90', 'p99.00', 'Average']\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Compare average throughput\n",
    "for size in payload_sizes:\n",
    "    for payload_bound in payload_bounds:\n",
    "        for circle_size in circle_sizes:\n",
    "            for metric in metrics:\n",
    "                plt.clf() # Reset plot\n",
    "                df_fixed = df[(df['payload_size'] == size) & (df['payload_bound'] == payload_bound) & (df['circle_size'] == circle_size)]\n",
    "\n",
    "                df_comp_fixed_temp = df_fixed.pivot(\"sender_type\", \"simult_req\", metric)\n",
    "                ax = df_comp_fixed_temp.plot(kind='bar', rot=0)\n",
    "\n",
    "                # Set plot title and labels\n",
    "                plt.title(f\"{metric} Latency comparison \\n for payload_size={size}, payload_bound={payload_bound}, circle_size={circle_size}\")\n",
    "                plt.xlabel('Protoype')\n",
    "                plt.ylabel(f\"{metric} Latency (ms)\")\n",
    "\n",
    "                # Orient sender_type labels at 45 degrees\n",
    "                ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "                ax.tick_params(axis='x', pad=10)\n",
    "\n",
    "                # Adjust plot layout\n",
    "                plt.legend(title='Simultaneous Requests', bbox_to_anchor=(1, 1))\n",
    "                plt.tight_layout()\n",
    "\n",
    "                # Show or save the plot\n",
    "                plt.savefig(f'plots/sender-comp-simult_req-{size}_{payload_bound}_{circle_size}_{metric}.svg', bbox_inches='tight', format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build \"over time\" plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for \"over time\" plots\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metric_over_time(time_series, metric_name, duration, y_axis_name, plot_filename):\n",
    "    metric_values_over_time = []\n",
    "    for t in range(duration):\n",
    "        metric_values_over_time.append(time_series[str(t)][metric_name])\n",
    "\n",
    "    plt.clf()\n",
    "    plt.bar(range(duration), metric_values_over_time)\n",
    "    plt.xlabel('Time (s)', fontsize=12)\n",
    "    plt.ylabel(y_axis_name, fontsize=12)\n",
    "    plt.xticks(fontsize=10, rotation=45, ha='right')\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.title(f'{y_axis_name} over Time', fontsize=14)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"plots/{plot_filename}.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Used for \"over time\" plots\n",
    "# Generate plots for all json\n",
    "sender_type = ['sender', 'sender_rdma', 'sender_rdma_write_multip_write', 'tcpproxy']\n",
    "payload_sizes = ['10-100','100-1000','1000-10000']\n",
    "simult_req = ['1','10','100']\n",
    "for size in payload_sizes:\n",
    "   for n in simult_req:     \n",
    "        data = []\n",
    "        for sender in sender_type:\n",
    "            result_json = f\"results/json_{sender}_{size}_{n}\"\n",
    "            print(result_json)\n",
    "            with open(result_json, 'r') as file:\n",
    "                result =  json.load(file)        \n",
    "\n",
    "            duration = result['configuration']['test_time']\n",
    "            time_series = result['ALL STATS']['Totals']['Time-Serie']\n",
    "\n",
    "            plot_metric_over_time(time_series=time_series, metric_name='p99.90', duration=duration, y_axis_name='p99.90 Latency (ms)', plot_filename=f'{sender}_{size}_{n}_p99.90')\n",
    "            plot_metric_over_time(time_series=time_series, metric_name='p99.00', duration=duration, y_axis_name='p99.00 Latency (ms)', plot_filename=f'{sender}_{size}_{n}_p99.00')\n",
    "            plot_metric_over_time(time_series=time_series, metric_name='p50.00', duration=duration, y_axis_name='p50.00 Latency (ms)', plot_filename=f'{sender}_{size}_{n}_p50.00')\n",
    "            plot_metric_over_time(time_series=time_series, metric_name='Average Latency', duration=duration, y_axis_name='Average Latency (ms)', plot_filename=f'{sender}_{size}_{n}_avg_latency')\n",
    "            plot_metric_over_time(time_series=time_series, metric_name='Count', duration=duration, y_axis_name='Count', plot_filename=f'{sender}_{size}_{n}_count')\n",
    "            plot_metric_over_time(time_series=time_series, metric_name='Max Latency', duration=duration, y_axis_name='Maximum Latency (ms)', plot_filename=f'{sender}_{size}_{n}_max_latency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate plots for individual json\n",
    "result_json = f\"results/json_rdma_sender_10-100_1\"\n",
    "print(result_json)          \n",
    "\n",
    "duration = result['configuration']['test_time']\n",
    "plot_metric_over_time(time_series=time_series, metric_name='p99.90', duration=duration, y_axis_name='p99.90 Latency (ms)', plot_filename=f'{sender}_{size}_{n}_p99.90')\n",
    "plot_metric_over_time(time_series=time_series, metric_name='p99.00', duration=duration, y_axis_name='p99.00 Latency (ms)', plot_filename=f'{sender}_{size}_{n}_p99.00')\n",
    "plot_metric_over_time(time_series=time_series, metric_name='p50.00', duration=duration, y_axis_name='p50.00 Latency (ms)', plot_filename=f'{sender}_{size}_{n}_p50.00')\n",
    "plot_metric_over_time(time_series=time_series, metric_name='Average Latency', duration=duration, y_axis_name='Average Latency (ms)', plot_filename=f'{sender}_{size}_{n}_avg_latency')\n",
    "plot_metric_over_time(time_series=time_series, metric_name='Count', duration=duration, y_axis_name='Count', plot_filename=f'{sender}_{size}_{n}_count')\n",
    "plot_metric_over_time(time_series=time_series, metric_name='Max Latency', duration=duration, y_axis_name='Maximum Latency (ms)', plot_filename=f'{sender}_{size}_{n}_max_latency')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
