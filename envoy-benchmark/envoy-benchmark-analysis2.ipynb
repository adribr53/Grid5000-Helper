{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Build csv files for all xps\n",
    "\n",
    "sender_type = ['sender_rdma', 'sender_rdma_write_multip_read']\n",
    "payload_sizes = ['10-100', '100-1000']\n",
    "simult_req = ['1']\n",
    "payload_bounds = ['1500']\n",
    "circle_sizes = ['200']\n",
    "n_runs = 5\n",
    "\n",
    "data = []\n",
    "\n",
    "for size in payload_sizes:\n",
    "   for n in simult_req:\n",
    "    for payload_bound in payload_bounds:\n",
    "            for circle_size in circle_sizes:\n",
    "        # one xp, let's get th latencies and avg req rates\n",
    "        #   CASE   OPS/SEC ...\n",
    "        # sender | res1 res2 res3        \n",
    "                #For custom senders\n",
    "                for sender in sender_type:\n",
    "                    ops_sec = []\n",
    "                    avg_latency = []\n",
    "                    p50_00_latency = []\n",
    "                    p99_00_latency = []\n",
    "                    p99_99_latency = []\n",
    "\n",
    "                    for i in range(n_runs):\n",
    "                        result_json = f\"results2/json_{sender}_{size}_{n}_{payload_bound}_{circle_size}_{i}\"\n",
    "                        # print(result_json)    \n",
    "                        with open(result_json, 'r') as file:\n",
    "                            result =  json.load(file)                \n",
    "                            \n",
    "                            # result_dict = {}\n",
    "                            # result_dict[\"sender_type\"] = sender[7:]\n",
    "                            # result_dict[\"payload_size\"] = size\n",
    "                            # result_dict[\"simult_req\"] = n\n",
    "                            # result_dict[\"payload_bound\"] = payload_bound\n",
    "                            # result_dict[\"circle_size\"] = circle_size\n",
    "                            ops_sec.append(result['ALL STATS']['Totals']['Ops/sec'])\n",
    "                            avg_latency.append(result['ALL STATS']['Totals']['Average Latency'])\n",
    "                            p50_00_latency.append(result['ALL STATS']['Totals']['Percentile Latencies']['p50.00'])\n",
    "                            p99_00_latency.append(result['ALL STATS']['Totals']['Percentile Latencies']['p99.00'])\n",
    "                            p99_99_latency.append(result['ALL STATS']['Totals']['Percentile Latencies']['p99.90'])\n",
    "                            \n",
    "                            # data.append(result_dict)\n",
    "                            # global_data.append(result_dict)\n",
    "                    # print(result)  \n",
    "                    result_dict = {}\n",
    "                    result_dict[\"sender_type\"] = sender[7:]\n",
    "                    result_dict[\"payload_size\"] = size\n",
    "                    result_dict[\"simult_req\"] = n\n",
    "                    result_dict[\"payload_bound\"] = payload_bound\n",
    "                    result_dict[\"circle_size\"] = circle_size\n",
    "\n",
    "                    result_dict[\"Ops/sec\"] = np.mean(ops_sec)\n",
    "                    result_dict[\"Average\"] = np.mean(avg_latency)\n",
    "                    result_dict[\"p50.00\"] = np.mean(p50_00_latency)\n",
    "                    result_dict[\"p99.00\"] = np.mean(p99_00_latency)\n",
    "                    result_dict[\"p99.90\"] = np.mean(p99_99_latency)\n",
    "\n",
    "                    result_dict[\"std_Ops/sec\"] = np.std(ops_sec)\n",
    "                    result_dict[\"std_Average\"] = np.std(avg_latency)\n",
    "                    result_dict[\"std_p50.00\"] = np.std(p50_00_latency)\n",
    "                    result_dict[\"std_p99.00\"] = np.std(p99_00_latency)\n",
    "                    result_dict[\"std_p99.90\"] = np.std(p99_99_latency)\n",
    "\n",
    "                    data.append(result_dict)\n",
    "                # #For tcp_proxy    \n",
    "                # result_json = f\"results/json_tcpproxy_{size}_{n}\"\n",
    "                # # print(result_json)            \n",
    "                # with open(result_json, 'r') as file:\n",
    "                #     result =  json.load(file)                \n",
    "                    \n",
    "                #     result_dict = {}\n",
    "                #     result_dict[\"sender_type\"] = \"tcpproxy\"\n",
    "                #     result_dict[\"payload_size\"] = size\n",
    "                #     result_dict[\"simult_req\"] = n\n",
    "                #     result_dict[\"payload_bound\"] = payload_bound\n",
    "                #     result_dict[\"circle_size\"] = circle_size\n",
    "                #     result_dict[\"Ops/sec\"] = result['ALL STATS']['Totals']['Ops/sec']\n",
    "                #     result_dict[\"Average\"] = result['ALL STATS']['Totals']['Average Latency']\n",
    "                #     result_dict[\"p50.00\"] = result['ALL STATS']['Totals']['Percentile Latencies']['p50.00']\n",
    "                #     result_dict[\"p99.00\"] = result['ALL STATS']['Totals']['Percentile Latencies']['p99.00']\n",
    "                #     result_dict[\"p99.90\"] = result['ALL STATS']['Totals']['Percentile Latencies']['p99.90']\n",
    "                    \n",
    "                #     data.append(result_dict)\n",
    "                #     global_data.append(result_dict)\n",
    "                # print(result)  \n",
    "\n",
    "                # print(\"\\n\\n\\n\")\n",
    "                # df = pd.DataFrame(data)\n",
    "                # df.to_csv(f'data2/res_{size}_{n}_{payload_bound}_{circle_size}.csv')\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(f'data2/res_global.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build (sender_type, payload_size) comparison plots\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('data2/res_global.csv')\n",
    "\n",
    "simult_req = [1]\n",
    "payload_bounds = [1500]\n",
    "circle_sizes = [200]\n",
    "metrics = ['p99.90']\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Compare average throughput\n",
    "for n in simult_req:\n",
    "    for payload_bound in payload_bounds:\n",
    "        for circle_size in circle_sizes:\n",
    "            for metric in metrics:\n",
    "                plt.clf() # Reset plot\n",
    "                df_fixed = df[(df['simult_req'] == n) & (df['payload_bound'] == payload_bound) & (df['circle_size'] == circle_size)]\n",
    "\n",
    "                df_comp_fixed_temp = df_fixed.pivot(\"sender_type\", \"payload_size\", metric)\n",
    "                yerr = df_fixed.pivot(\"sender_type\", \"payload_size\", f\"std_{metric}\")\n",
    "                ax = df_comp_fixed_temp.plot(kind='bar', rot=0, yerr=yerr, capsize=4)\n",
    "\n",
    "                # Set plot title and labels\n",
    "                plt.title(f\"{metric} Latency comparison \\n for simult_req={n}, payload_bound={payload_bound}, circle_size={circle_size}\")\n",
    "                plt.xlabel('Protoype')\n",
    "                plt.ylabel(f\"{metric} Latency (ms)\")\n",
    "\n",
    "                # Orient sender_type labels at 45 degrees\n",
    "                ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "                ax.tick_params(axis='x', pad=10)\n",
    "\n",
    "                # Adjust plot layout\n",
    "                plt.legend(title='Payload Size', bbox_to_anchor=(1, 1))\n",
    "                plt.tight_layout()\n",
    "\n",
    "                # Show or save the plot\n",
    "                # plt.savefig(f'plots/sender-comp-payload_size-{n}_{payload_bound}_{circle_size}_{metric}.svg', bbox_inches='tight', format='svg')\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Build csv files for all xps\n",
    "\n",
    "sender_type = ['sender_rdma', 'sender_rdma_write_multip_write', 'sender_rdma_write_multip_read', 'sender_rdma_write_nomultip_read', 'tcpproxy']\n",
    "payload_sizes = ['10-100','100-1000','1000-10000']\n",
    "simult_req = ['1','10','100']\n",
    "payload_bounds = ['1500']\n",
    "circle_sizes = ['200']\n",
    "n_runs = 5\n",
    "data = []\n",
    "\n",
    "for size in payload_sizes:\n",
    "   for n in simult_req:\n",
    "    for payload_bound in payload_bounds:\n",
    "        for circle_size in circle_sizes:\n",
    "            for sender in sender_type:\n",
    "                ops_sec = []\n",
    "                avg_latency = []\n",
    "                p50_00_latency = []\n",
    "                p99_00_latency = []\n",
    "                p99_99_latency = []\n",
    "\n",
    "                for i in range(n_runs):\n",
    "                    if sender != \"tcpproxy\":\n",
    "                        result_json = f\"results/json_{sender}_{size}_{n}_{payload_bound}_{circle_size}_{i}\"\n",
    "                    else:\n",
    "                        result_json = f\"results/json_tcpproxy_{size}_{n}\"\n",
    "                    # print(result_json)    \n",
    "                    with open(result_json, 'r') as file:\n",
    "                        result =  json.load(file)                \n",
    "                        \n",
    "                        ops_sec.append(result['ALL STATS']['Totals']['Ops/sec'])\n",
    "                        avg_latency.append(result['ALL STATS']['Totals']['Average Latency'])\n",
    "                        p50_00_latency.append(result['ALL STATS']['Totals']['Percentile Latencies']['p50.00'])\n",
    "                        p99_00_latency.append(result['ALL STATS']['Totals']['Percentile Latencies']['p99.00'])\n",
    "                        p99_99_latency.append(result['ALL STATS']['Totals']['Percentile Latencies']['p99.90'])\n",
    "                        \n",
    "                result_dict = {}\n",
    "                result_dict[\"sender_type\"] = sender\n",
    "                result_dict[\"payload_size\"] = size\n",
    "                result_dict[\"simult_req\"] = n\n",
    "                result_dict[\"payload_bound\"] = payload_bound\n",
    "                result_dict[\"circle_size\"] = circle_size\n",
    "\n",
    "                result_dict[\"Ops/sec\"] = np.mean(ops_sec)\n",
    "                result_dict[\"Average\"] = np.mean(avg_latency)\n",
    "                result_dict[\"p50.00\"] = np.mean(p50_00_latency)\n",
    "                result_dict[\"p99.00\"] = np.mean(p99_00_latency)\n",
    "                result_dict[\"p99.90\"] = np.mean(p99_99_latency)\n",
    "\n",
    "                result_dict[\"std_Ops/sec\"] = np.std(ops_sec)\n",
    "                result_dict[\"std_Average\"] = np.std(avg_latency)\n",
    "                result_dict[\"std_p50.00\"] = np.std(p50_00_latency)\n",
    "                result_dict[\"std_p99.00\"] = np.std(p99_00_latency)\n",
    "                result_dict[\"std_p99.90\"] = np.std(p99_99_latency)\n",
    "\n",
    "                data.append(result_dict)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(f'data/res_global.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build (sender_type, payload_size) comparison plots\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('data/res_global.csv')\n",
    "\n",
    "simult_req = [1, 10, 100]\n",
    "payload_bounds = [1500]\n",
    "circle_sizes = [200]\n",
    "metrics = ['p99.90', 'p99.00', 'p50.00', 'Average']\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Compare average throughput\n",
    "for n in simult_req:\n",
    "    for payload_bound in payload_bounds:\n",
    "        for circle_size in circle_sizes:\n",
    "            for metric in metrics:\n",
    "                plt.clf() # Reset plot\n",
    "                df_fixed = df[(df['simult_req'] == n) & (df['payload_bound'] == payload_bound) & (df['circle_size'] == circle_size)]\n",
    "\n",
    "                df_comp_fixed_temp = df_fixed.pivot(\"sender_type\", \"payload_size\", metric)\n",
    "                yerr = df_fixed.pivot(\"sender_type\", \"payload_size\", f\"std_{metric}\")\n",
    "                ax = df_comp_fixed_temp.plot(kind='bar', rot=0, yerr=yerr, capsize=4)\n",
    "\n",
    "                # Set plot title and labels\n",
    "                plt.title(f\"{metric} Latency comparison \\n for simult_req={n}, payload_bound={payload_bound}, circle_size={circle_size}\")\n",
    "                plt.xlabel('Protoype')\n",
    "                plt.ylabel(f\"{metric} Latency (ms)\")\n",
    "\n",
    "                # Orient sender_type labels at 45 degrees\n",
    "                ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "                ax.tick_params(axis='x', pad=10)\n",
    "\n",
    "                # Adjust plot layout\n",
    "                plt.legend(title='Payload Size', bbox_to_anchor=(1, 1))\n",
    "                plt.tight_layout()\n",
    "\n",
    "                # Save the plot\n",
    "                plt.savefig(f'plots/sender-comp-payload_size-{n}_{payload_bound}_{circle_size}_{metric}.svg', bbox_inches='tight', format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build (sender_type, simult_req) comparison plots\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('data/res_global.csv')\n",
    "\n",
    "payload_sizes = ['10-100','100-1000','1000-10000']\n",
    "payload_bounds = [1500]\n",
    "circle_sizes = [200]\n",
    "metrics = ['p99.90', 'p99.00', 'p50.00', 'Average']\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Compare average throughput\n",
    "for size in payload_sizes:\n",
    "    for payload_bound in payload_bounds:\n",
    "        for circle_size in circle_sizes:\n",
    "            for metric in metrics:\n",
    "                plt.clf() # Reset plot\n",
    "                df_fixed = df[(df['payload_size'] == size) & (df['payload_bound'] == payload_bound) & (df['circle_size'] == circle_size)]\n",
    "\n",
    "                df_comp_fixed_temp = df_fixed.pivot(\"sender_type\", \"simult_req\", metric)\n",
    "                yerr = df_fixed.pivot(\"sender_type\", \"simult_req\", f\"std_{metric}\")\n",
    "                ax = df_comp_fixed_temp.plot(kind='bar', rot=0, yerr=yerr, capsize=4)\n",
    "\n",
    "                # Set plot title and labels\n",
    "                plt.title(f\"{metric} Latency comparison \\n for payload_size={size}, payload_bound={payload_bound}, circle_size={circle_size}\")\n",
    "                plt.xlabel('Protoype')\n",
    "                plt.ylabel(f\"{metric} Latency (ms)\")\n",
    "\n",
    "                # Orient sender_type labels at 45 degrees\n",
    "                ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "                ax.tick_params(axis='x', pad=10)\n",
    "\n",
    "                # Adjust plot layout\n",
    "                plt.legend(title='Simultaneous Requests', bbox_to_anchor=(1, 1))\n",
    "                plt.tight_layout()\n",
    "\n",
    "                # Save the plot\n",
    "                plt.savefig(f'plots/sender-comp-simult_req-{size}_{payload_bound}_{circle_size}_{metric}.svg', bbox_inches='tight', format='svg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
